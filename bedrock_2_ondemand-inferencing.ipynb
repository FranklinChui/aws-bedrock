{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Amazon Bedrock\n",
        "\n",
        "Bedrock model inferencing using boto3 & langchain\n",
        "\n",
        "After some tinkling, you do not need \"Provisioned throughput\". As long as the model access is enabled and the right IAM policy is in place, inferencing via AWS credentials is straight forward.\n",
        "\n",
        "I will update again when I got a clear picture how much I will be charged. The early \"Provisioned Throughput\" do cost a bump. :(\n",
        "\n",
        "### Pre-requisites\n",
        "\n",
        "#### AWS Security Credentials\n",
        "\n",
        "**User Security Token**\n",
        "- Ensure your user security `Access Key` and `Secret Key ID` is enabled in AWS Security & Credential console\n",
        "- Ensure the these keys are setup in respective enivironment (colab/kaggle/vscode via .env)\n",
        "\n",
        "**IAM Policy**\n",
        "\n",
        "Ensure the following policy is added to the user's IAM Role for model inferencing.\n",
        "\n",
        "[[Reference](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-prereq.html)]\n",
        "\n",
        "```python\n",
        "IAM_POLICY=\"\"\"\n",
        "{\n",
        "    \"Version\": \"2012-10-17\",\n",
        "    \"Statement\": [\n",
        "        {\n",
        "            \"Sid\": \"ModelInvocationPermissions\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"bedrock:InvokeModel\",\n",
        "                \"bedrock:InvokeModelWithResponseStream\",\n",
        "                \"bedrock:GetInferenceProfile\",\n",
        "                \"bedrock:ListInferenceProfiles\",\n",
        "                \"bedrock:RenderPrompt\",\n",
        "                \"bedrock:GetCustomModel\",\n",
        "                \"bedrock:ListCustomModels\",\n",
        "                \"bedrock:GetImportedModel\",\n",
        "                \"bedrock:ListImportedModels\",\n",
        "                \"bedrock:GetProvisionedModelThroughput\",\n",
        "                \"bedrock:ListProvisionedModelThroughputs\",\n",
        "                \"bedrock:GetGuardrail\",\n",
        "                \"bedrock:ListGuardrails\",\n",
        "                \"bedrock:ApplyGuardrail\"\n",
        "            ],\n",
        "            \"Resource\": \"arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3*\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "**Bedrock model access** to desire model(s) are setup\n",
        "\n",
        "#### \"Provisioned throughput\n",
        "\n",
        "Provisioned Throughput to provision is for a higher level of throughput for a model at a fixed cost. **Customized model** will require to purchase Provisioned Throughput to be able to use it.\n",
        "\n",
        "> very important: remember to <u>delete immediately</u> after use (just in case)\n",
        "> https://aws.amazon.com/bedrock/pricing/\n",
        "\n",
        "[supported models](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-supported.html) by region in AWS\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lts_1h6rSFa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries Setup"
      ],
      "metadata": {
        "id": "vM9tDOKxmc4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain boto3 langchain-aws anthropic"
      ],
      "metadata": {
        "id": "p2JN3Az5SEjL"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "os.environ['AWS_REGION'] = userdata.get('AWS_REGION')\n",
        "os.environ['AWS_ACCOUNT_ID'] = userdata.get('AWS_ACCOUNT_ID')"
      ],
      "metadata": {
        "id": "Kk8TVqfLLyyN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using boto3"
      ],
      "metadata": {
        "id": "soxAhFhMgL_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "bedrock_runtime = boto3.client(\n",
        "    service_name='bedrock-runtime',\n",
        "    region_name='us-east-1')\n",
        "\n",
        "prompt = \"What is the capital of France?\"\n",
        "\n",
        "# original intended for claude 3.5 haiku but this model is available for \"Provisioned throughput\"\n",
        "# kwargs = {\n",
        "#   \"modelId\": \"anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
        "#   \"contentType\": \"application/json\",\n",
        "#   \"accept\": \"application/json\",\n",
        "#   \"body\": json.dumps({\n",
        "#     \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "#     \"max_tokens\": 200,\n",
        "#     \"top_k\": 250,\n",
        "#     \"stopSequences\": [],\n",
        "#     \"temperature\": 1,\n",
        "#     \"top_p\": 0.999,\n",
        "#     \"messages\": [\n",
        "#       {\n",
        "#         \"role\": \"user\",\n",
        "#         \"content\": [\n",
        "#           {\n",
        "#             \"type\": \"text\",\n",
        "#             \"text\": prompt\n",
        "#           }\n",
        "#         ]\n",
        "#       }\n",
        "#     ]\n",
        "#   })\n",
        "# }\n",
        "\n",
        "kwargs = {\n",
        "  \"modelId\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "  \"contentType\": \"application/json\",\n",
        "  \"accept\": \"application/json\",\n",
        "  \"body\": json.dumps({\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 1000,\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": prompt\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ]\n",
        "  })\n",
        "}\n",
        "\n",
        "response = bedrock_runtime.invoke_model(**kwargs)\n",
        "response_body = json.loads(response.get('body').read())\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(response_body)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJTUg3ZyJnTm",
        "outputId": "abc117bd-1e98-4018-8290-20e4b748bc4e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'content': [{'text': 'The capital of France is Paris.', 'type': 'text'}],\n",
            " 'id': 'msg_bdrk_016wCsryaQDWaW2AriPgs9TK',\n",
            " 'model': 'claude-3-haiku-20240307',\n",
            " 'role': 'assistant',\n",
            " 'stop_reason': 'end_turn',\n",
            " 'stop_sequence': None,\n",
            " 'type': 'message',\n",
            " 'usage': {'input_tokens': 14, 'output_tokens': 10}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(response_body['content'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqTA34aCX4TP",
        "outputId": "922d5d4f-da2a-4b8b-cbb0-41414264993f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'The capital of France is Paris.', 'type': 'text'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using boto3"
      ],
      "metadata": {
        "id": "uu_kdlj7gQ07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
        "    aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
        "    region_name=os.environ['AWS_REGION']\n",
        ")\n",
        "\n",
        "bedrock_runtime = session.client('bedrock-runtime')\n",
        "\n",
        "# For LangChain integration\n",
        "from langchain_aws import ChatBedrock\n",
        "\n",
        "llm = ChatBedrock(\n",
        "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "    client=bedrock_runtime,\n",
        "    model_kwargs={\n",
        "        \"temperature\": 0.5,\n",
        "        \"max_tokens\": 1000\n",
        "    }\n",
        ")\n",
        "\n",
        "response = bedrock_runtime.invoke_model(\n",
        "    modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "    body=json.dumps({\n",
        "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "        \"max_tokens\": 1000,\n",
        "        \"messages\": [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain quantum computing in simple terms\"\n",
        "        }]\n",
        "    })\n",
        ")\n",
        "response_body = json.loads(response.get('body').read())\n",
        "pprint(response_body)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljF2_imOhTKc",
        "outputId": "b3ddb024-b04d-46c0-de19-a912acbad629"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'content': [{'text': 'Quantum computing is a revolutionary computing '\n",
            "                      'technology that harnesses the principles of quantum '\n",
            "                      'mechanics, which govern the behavior of particles at '\n",
            "                      'the atomic and subatomic levels. In simple terms, '\n",
            "                      \"here's how it works:\\n\"\n",
            "                      '\\n'\n",
            "                      '1. Qubits: In classical computing, the basic unit of '\n",
            "                      'information is a bit, which can exist in either a 0 or '\n",
            "                      'a 1 state. In quantum computing, the basic unit is '\n",
            "                      'called a qubit (quantum bit), which can exist in a '\n",
            "                      'combination of 0 and 1 states simultaneously, a '\n",
            "                      'phenomenon known as superposition.\\n'\n",
            "                      '\\n'\n",
            "                      '2. Superposition: This superposition allows qubits to '\n",
            "                      'represent and process multiple states simultaneously, '\n",
            "                      \"which is not possible in classical computing. It's like \"\n",
            "                      'having a coin that can be both heads and tails at the '\n",
            "                      'same time, until you observe it and force it into one '\n",
            "                      'state or the other.\\n'\n",
            "                      '\\n'\n",
            "                      '3. Entanglement: Qubits can also exhibit a phenomenon '\n",
            "                      'called entanglement, where the state of one qubit is '\n",
            "                      'intrinsically linked to the state of another qubit, '\n",
            "                      'even if they are physically separated. This allows '\n",
            "                      'quantum computers to process information in a highly '\n",
            "                      'parallel and interconnected way, enabling them to solve '\n",
            "                      'certain types of problems much faster than classical '\n",
            "                      'computers.\\n'\n",
            "                      '\\n'\n",
            "                      '4. Quantum algorithms: Quantum computers use '\n",
            "                      'specialized algorithms that take advantage of the '\n",
            "                      'quantum properties of qubits, such as superposition and '\n",
            "                      'entanglement, to perform calculations and solve '\n",
            "                      'problems more efficiently than classical computers.\\n'\n",
            "                      '\\n'\n",
            "                      '5. Potential applications: Quantum computing has the '\n",
            "                      'potential to revolutionize various fields, including '\n",
            "                      'cryptography, optimization problems, simulations of '\n",
            "                      'quantum systems (e.g., for material science and '\n",
            "                      'chemistry), machine learning, and more. Tasks that '\n",
            "                      'would take classical computers an impractically long '\n",
            "                      'time could be solved much faster by quantum computers.\\n'\n",
            "                      '\\n'\n",
            "                      'While quantum computing is still in its early stages '\n",
            "                      'and faces challenges in terms of scaling and error '\n",
            "                      'correction, it represents a paradigm shift in computing '\n",
            "                      'and has the potential to tackle complex problems that '\n",
            "                      'are practically impossible for classical computers to '\n",
            "                      'solve efficiently.',\n",
            "              'type': 'text'}],\n",
            " 'id': 'msg_bdrk_01TBBB3CeSKLUzN7idGNfZA7',\n",
            " 'model': 'claude-3-sonnet-20240229',\n",
            " 'role': 'assistant',\n",
            " 'stop_reason': 'end_turn',\n",
            " 'stop_sequence': None,\n",
            " 'type': 'message',\n",
            " 'usage': {'input_tokens': 14, 'output_tokens': 428}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
        "    aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
        "    region_name=os.environ['AWS_REGION']\n",
        ")\n",
        "\n",
        "bedrock_runtime = session.client('bedrock-runtime')\n",
        "\n",
        "# For LangChain integration\n",
        "from langchain_aws import ChatBedrock\n",
        "\n",
        "llm = ChatBedrock(\n",
        "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "    client=bedrock_runtime,\n",
        "    model_kwargs={\n",
        "        \"temperature\": 0.5,\n",
        "        \"max_tokens\": 1000\n",
        "    }\n",
        ")\n",
        "\n",
        "response = bedrock_runtime.invoke_model(\n",
        "    modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "    body=json.dumps({\n",
        "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "        \"max_tokens\": 1000,\n",
        "        \"messages\": [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain GPT transformer in 50 words to a 5 year old.\"\n",
        "        }]\n",
        "    })\n",
        ")\n",
        "response_body = json.loads(response.get('body').read())\n",
        "pprint(response_body)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJnWKAb0pHvV",
        "outputId": "4f9946da-3c02-4a51-bc4a-5e19f1b651ff"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'content': [{'text': 'GPT transformer is like a smart machine that can '\n",
            "                      'understand and create language like humans. It works by '\n",
            "                      'breaking down texts into small pieces, understanding '\n",
            "                      'their meanings, and then putting them back together in '\n",
            "                      \"new and creative ways. It's like a puzzle-solving robot \"\n",
            "                      'that can understand and build sentences.',\n",
            "              'type': 'text'}],\n",
            " 'id': 'msg_bdrk_01543aaZrCsDBGwi9vZiQBh1',\n",
            " 'model': 'claude-3-sonnet-20240229',\n",
            " 'role': 'assistant',\n",
            " 'stop_reason': 'end_turn',\n",
            " 'stop_sequence': None,\n",
            " 'type': 'message',\n",
            " 'usage': {'input_tokens': 25, 'output_tokens': 61}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Langchain"
      ],
      "metadata": {
        "id": "UdOl-hm_o9xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cost(input_tokens, output_tokens):\n",
        "    input_cost = (input_tokens / 1000) * 0.003\n",
        "    output_cost = (output_tokens / 1000) * 0.015\n",
        "    return round(input_cost + output_cost, 4)\n",
        "\n"
      ],
      "metadata": {
        "id": "xB-Pxtvt7mrl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage cost:\n",
        "cost = calculate_cost(\n",
        "    response_body['usage']['input_tokens'],\n",
        "    response_body['usage']['output_tokens']\n",
        ")\n",
        "print(f\"Cost: ${cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS5nYrWz7r2n",
        "outputId": "243e6c73-78e2-44a3-d211-888b3f26ca2d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost: $0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm\n",
        "response = chain.invoke({\"input\": \"Explain serverless architecture in 50 words to 80 year old\"})"
      ],
      "metadata": {
        "id": "6D2Rs9RLoXst"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "pprint(response)\n",
        "print(type(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rLBCJVcoaTi",
        "outputId": "eebea4be-407b-4d85-9189-6d8a61529fb7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AIMessage(content='Serverless architecture is like having a personal chef who cooks your meals for you. You don\\'t have to worry about the kitchen or the equipment, you just tell the chef what you want, and they take care of it. The chef is the \"server\" and you are the \"client\" who gets the food (or service) without the hassle.', additional_kwargs={'usage': {'prompt_tokens': 28, 'completion_tokens': 77, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 105}, 'stop_reason': 'end_turn', 'thinking': {}, 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0', 'model_name': 'anthropic.claude-3-haiku-20240307-v1:0'}, response_metadata={'usage': {'prompt_tokens': 28, 'completion_tokens': 77, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 105}, 'stop_reason': 'end_turn', 'thinking': {}, 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0', 'model_name': 'anthropic.claude-3-haiku-20240307-v1:0'}, id='run-265d25e9-d684-4dce-b322-0afb4cf9d4db-0', usage_metadata={'input_tokens': 28, 'output_tokens': 77, 'total_tokens': 105, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})\n",
            "<class 'langchain_core.messages.ai.AIMessage'>\n"
          ]
        }
      ]
    }
  ]
}